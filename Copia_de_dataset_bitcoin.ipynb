{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GiMQWnXyxPnH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar dataset (ajusta el path si es necesario)\n",
        "df = pd.read_csv(\"/content/btcusd_1-min_data.csv\")\n",
        "\n",
        "# Preprocesamiento de fecha y cierre\n",
        "df['Date'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "df = df.set_index('Date')[['Close']].copy()\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Escalado\n",
        "scaler = MinMaxScaler()\n",
        "scaled_close = scaler.fit_transform(df[['Close']].values).flatten()"
      ],
      "metadata": {
        "id": "FcP7Vat8FYBZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para crear secuencias\n",
        "def create_sequences(data, seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_len):\n",
        "        X.append(data[i:i+seq_len])\n",
        "        y.append(data[i+seq_len])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Modelo LSTM\n",
        "class BitcoinLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(BitcoinLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])"
      ],
      "metadata": {
        "id": "UGEDQ8OsFcoz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "def train_model(model, X_train, y_train, X_val, y_val, num_epochs, lr):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_val)\n",
        "        val_loss = criterion(preds, y_val).item()\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "kogmQkkJFgCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import random\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def genetic_algorithm(y, pop_size=4, generations=2, max_samples=100_000):\n",
        "    population = []\n",
        "\n",
        "    # Crear población inicial\n",
        "    for _ in range(pop_size):\n",
        "        individual = {\n",
        "            'seq_len': random.randint(10, 50),\n",
        "            'hidden_size': random.choice([16, 32, 64, 128]),\n",
        "            'num_layers': random.choice([1, 2, 3]),\n",
        "            'dropout': random.uniform(0.0, 0.5),\n",
        "            'lr': random.uniform(0.0005, 0.01)\n",
        "        }\n",
        "        if individual['num_layers'] == 1:\n",
        "            individual['dropout'] = 0.0\n",
        "        population.append(individual)\n",
        "\n",
        "    for gen in range(generations):\n",
        "        fitness_scores = []\n",
        "\n",
        "        for ind in population:\n",
        "            seq_len = ind['seq_len']\n",
        "\n",
        "            # Limitar tamaño del dataset para no saturar la RAM\n",
        "            y_sample = y[:max_samples]\n",
        "\n",
        "            # Crear secuencias\n",
        "            X, y_seq = create_sequences(y_sample, seq_len)\n",
        "            if len(X) < 500:\n",
        "                continue\n",
        "\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "            # Convertir a tensores\n",
        "            X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "            y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "            X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "            y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "            # Crear y entrenar modelo\n",
        "            model = BitcoinLSTM(1, ind['hidden_size'], ind['num_layers'], ind['dropout'])\n",
        "            loss = train_model(model, X_train, y_train, X_val, y_val, num_epochs=2, lr=ind['lr'])\n",
        "            fitness_scores.append((loss, ind))\n",
        "\n",
        "            # Liberar memoria\n",
        "            del model, X_train, y_train, X_val, y_val\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        if len(fitness_scores) < 2:\n",
        "            print(\"⚠️ Muy pocos individuos válidos, intenta aumentar `pop_size` o mejorar `create_sequences`.\")\n",
        "            break\n",
        "\n",
        "        # Seleccionar los mejores\n",
        "        fitness_scores.sort(key=lambda x: x[0])\n",
        "        best = fitness_scores[:max(2, pop_size // 2)]\n",
        "\n",
        "        new_population = [ind for _, ind in best]\n",
        "\n",
        "        # Cruce y mutación\n",
        "        while len(new_population) < pop_size:\n",
        "            if len(new_population) >= 2:\n",
        "                p1, p2 = random.sample(new_population, 2)\n",
        "            else:\n",
        "                p1 = p2 = new_population[0]\n",
        "\n",
        "            child = {\n",
        "                'seq_len': random.choice([p1['seq_len'], p2['seq_len']]),\n",
        "                'hidden_size': random.choice([p1['hidden_size'], p2['hidden_size']]),\n",
        "                'num_layers': random.choice([p1['num_layers'], p2['num_layers']]),\n",
        "                'dropout': max(0.0, min(0.5, (p1['dropout'] + p2['dropout']) / 2 + random.uniform(-0.05, 0.05))),\n",
        "                'lr': max(0.0001, min(0.01, (p1['lr'] + p2['lr']) / 2 + random.uniform(-0.001, 0.001)))\n",
        "            }\n",
        "\n",
        "            if child['num_layers'] == 1:\n",
        "                child['dropout'] = 0.0\n",
        "\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "        print(f\"🧬 Gen {gen+1}/{generations}, Mejor pérdida: {best[0][0]:.6f}\")\n",
        "\n",
        "    return fitness_scores[0][1]\n"
      ],
      "metadata": {
        "id": "joulcBzQQrh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import gc\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# def genetic_algorithm(y, pop_size=4, generations=2, max_samples=5000):\n",
        "#     population = []\n",
        "#     for _ in range(pop_size):\n",
        "#         individual = {\n",
        "#             'seq_len': random.randint(10, 30),\n",
        "#             'hidden_size': random.choice([16, 32]),\n",
        "#             'num_layers': random.choice([1, 2]),\n",
        "#             'dropout': random.uniform(0.0, 0.3),\n",
        "#             'lr': random.uniform(0.001, 0.005)\n",
        "#         }\n",
        "#         population.append(individual)\n",
        "\n",
        "#     for gen in range(generations):\n",
        "#         fitness_scores = []\n",
        "\n",
        "#         for ind in population:\n",
        "#             try:\n",
        "#                 seq_len = ind['seq_len']\n",
        "#                 X, y_seq = create_sequences(y, seq_len)\n",
        "\n",
        "#                 # Limitar el tamaño total de muestras\n",
        "#                 if len(X) > max_samples:\n",
        "#                     indices = np.random.choice(len(X), size=max_samples, replace=False)\n",
        "#                     X = X[indices]\n",
        "#                     y_seq = y_seq[indices]\n",
        "\n",
        "#                 if len(X) < 300:\n",
        "#                     continue\n",
        "\n",
        "#                 X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "#                 # Tensores\n",
        "#                 X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "#                 y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "#                 X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "#                 y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "#                 # Crear modelo\n",
        "#                 model = BitcoinLSTM(1, ind['hidden_size'], ind['num_layers'], ind['dropout'])\n",
        "#                 loss = train_model(model, X_train, y_train, X_val, y_val, num_epochs=2, lr=ind['lr'])\n",
        "\n",
        "#                 fitness_scores.append((loss, ind))\n",
        "\n",
        "#                 # Liberar todo\n",
        "#                 del model, X_train, y_train, X_val, y_val\n",
        "#                 gc.collect()\n",
        "#                 torch.cuda.empty_cache()\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"❌ Error con individuo {ind}: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#         fitness_scores.sort(key=lambda x: x[0])\n",
        "#         best = fitness_scores[:pop_size//2]\n",
        "#         new_population = [ind for _, ind in best]\n",
        "\n",
        "#         while len(new_population) < pop_size:\n",
        "#             p1, p2 = random.sample(new_population, 2)\n",
        "#             child = {\n",
        "#                 'seq_len': random.choice([p1['seq_len'], p2['seq_len']]),\n",
        "#                 'hidden_size': random.choice([p1['hidden_size'], p2['hidden_size']]),\n",
        "#                 'num_layers': random.choice([p1['num_layers'], p2['num_layers']]),\n",
        "#                 'dropout': max(0.0, min(0.5, (p1['dropout'] + p2['dropout']) / 2 + random.uniform(-0.05, 0.05))),\n",
        "#                 'lr': max(0.0001, min(0.01, (p1['lr'] + p2['lr']) / 2 + random.uniform(-0.001, 0.001)))\n",
        "#             }\n",
        "#             new_population.append(child)\n",
        "\n",
        "#         population = new_population\n",
        "#         print(f\"🧬 Gen {gen+1}/{generations}, mejor pérdida: {best[0][0]:.6f}\")\n",
        "\n",
        "#     return fitness_scores[0][1]\n"
      ],
      "metadata": {
        "id": "AYR_gNeQQGOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# import torch\n",
        "\n",
        "# def genetic_algorithm(y, pop_size=10, generations=5):\n",
        "#     population = []\n",
        "#     for _ in range(pop_size):\n",
        "#         individual = {\n",
        "#             'seq_len': random.randint(10, 50),\n",
        "#             'hidden_size': random.choice([16, 32, 64]),\n",
        "#             'num_layers': random.choice([1, 2]),\n",
        "#             'dropout': random.uniform(0.0, 0.3),\n",
        "#             'lr': random.uniform(0.001, 0.005)\n",
        "#         }\n",
        "#         population.append(individual)\n",
        "\n",
        "#     for gen in range(generations):\n",
        "#         fitness_scores = []\n",
        "\n",
        "#         for ind in population:\n",
        "#             try:\n",
        "#                 seq_len = ind['seq_len']\n",
        "#                 X, y_seq = create_sequences(y, seq_len)\n",
        "#                 if len(X) < 500:\n",
        "#                     continue\n",
        "\n",
        "#                 # Liberar memoria explícitamente antes de cada iteración\n",
        "#                 gc.collect()\n",
        "#                 torch.cuda.empty_cache()\n",
        "\n",
        "#                 # Dividir datos\n",
        "#                 X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "#                 # Crear tensores sin ocupar más memoria de la necesaria\n",
        "#                 X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "#                 y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "#                 X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "#                 y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "#                 model = BitcoinLSTM(1, ind['hidden_size'], ind['num_layers'], ind['dropout'])\n",
        "#                 loss = train_model(model, X_train, y_train, X_val, y_val, num_epochs=3, lr=ind['lr'])\n",
        "#                 fitness_scores.append((loss, ind))\n",
        "\n",
        "#                 # Borrar todo lo pesado\n",
        "#                 del model, X_train, y_train, X_val, y_val\n",
        "#                 gc.collect()\n",
        "#                 torch.cuda.empty_cache()\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error con individuo {ind}: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#         fitness_scores.sort(key=lambda x: x[0])\n",
        "#         best = fitness_scores[:pop_size//2]\n",
        "#         new_population = [ind for _, ind in best]\n",
        "\n",
        "#         while len(new_population) < pop_size:\n",
        "#             parent1, parent2 = random.sample(new_population, 2)\n",
        "#             child = {\n",
        "#                 'seq_len': random.choice([parent1['seq_len'], parent2['seq_len']]),\n",
        "#                 'hidden_size': random.choice([parent1['hidden_size'], parent2['hidden_size']]),\n",
        "#                 'num_layers': random.choice([parent1['num_layers'], parent2['num_layers']]),\n",
        "#                 'dropout': max(0.0, min(0.5, (parent1['dropout'] + parent2['dropout']) / 2 + random.uniform(-0.05, 0.05))),\n",
        "#                 'lr': max(0.0001, min(0.01, (parent1['lr'] + parent2['lr']) / 2 + random.uniform(-0.001, 0.001)))\n",
        "#             }\n",
        "#             new_population.append(child)\n",
        "\n",
        "#         population = new_population\n",
        "#         print(f\"🧬 Generación {gen+1}/{generations}, Mejor pérdida: {best[0][0]:.6f}\")\n",
        "\n",
        "#     return fitness_scores[0][1]\n"
      ],
      "metadata": {
        "id": "dPo9qZqwLdwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# import torch\n",
        "# import random\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# def genetic_algorithm(y, pop_size=6, generations=3):\n",
        "#     population = []\n",
        "\n",
        "#     # Crear población inicial\n",
        "#     for _ in range(pop_size):\n",
        "#         individual = {\n",
        "#             'seq_len': random.randint(10, 30),  # rango más pequeño\n",
        "#             'hidden_size': random.choice([16, 32, 64]),\n",
        "#             'num_layers': random.choice([1, 2]),\n",
        "#             'dropout': round(random.uniform(0.0, 0.4), 2),\n",
        "#             'lr': round(random.uniform(0.0005, 0.005), 5)\n",
        "#         }\n",
        "#         population.append(individual)\n",
        "\n",
        "#     for gen in range(generations):\n",
        "#         fitness_scores = []\n",
        "\n",
        "#         for ind in population:\n",
        "#             seq_len = ind['seq_len']\n",
        "#             X, y_seq = create_sequences(y, seq_len)\n",
        "#             if len(X) < 500:\n",
        "#                 continue  # Evita conjuntos muy pequeños\n",
        "\n",
        "#             X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "#             # Convertir a tensores (agregamos la dimensión del canal)\n",
        "#             X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "#             y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "#             X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "#             y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "#             model = BitcoinLSTM(1, ind['hidden_size'], ind['num_layers'], ind['dropout'])\n",
        "\n",
        "#             # Entrenar modelo\n",
        "#             loss = train_model(model, X_train, y_train, X_val, y_val, num_epochs=3, lr=ind['lr'])\n",
        "\n",
        "#             fitness_scores.append((loss, ind))\n",
        "\n",
        "#             # 🔥 Liberar memoria usada por tensores y modelo\n",
        "#             del model, X_train, y_train, X_val, y_val\n",
        "#             gc.collect()\n",
        "#             torch.cuda.empty_cache()  # si estás usando GPU\n",
        "\n",
        "#         # Selección: los mejores sobrevivientes\n",
        "#         fitness_scores.sort(key=lambda x: x[0])\n",
        "#         best = fitness_scores[:pop_size // 2]\n",
        "#         new_population = [ind for _, ind in best]\n",
        "\n",
        "#         # Cruce y mutación\n",
        "#         while len(new_population) < pop_size:\n",
        "#             parent1, parent2 = random.sample(new_population, 2)\n",
        "#             child = {\n",
        "#                 'seq_len': random.choice([parent1['seq_len'], parent2['seq_len']]),\n",
        "#                 'hidden_size': random.choice([parent1['hidden_size'], parent2['hidden_size']]),\n",
        "#                 'num_layers': random.choice([parent1['num_layers'], parent2['num_layers']]),\n",
        "#                 'dropout': max(0.0, min(0.5, (parent1['dropout'] + parent2['dropout']) / 2 + random.uniform(-0.05, 0.05))),\n",
        "#                 'lr': max(0.0001, min(0.01, (parent1['lr'] + parent2['lr']) / 2 + random.uniform(-0.001, 0.001)))\n",
        "#             }\n",
        "#             new_population.append(child)\n",
        "\n",
        "#         population = new_population\n",
        "#         print(f\"🧬 Generación {gen + 1}/{generations} - Mejor pérdida: {best[0][0]:.6f}\")\n",
        "\n",
        "#     return fitness_scores[0][1]  # Mejor individuo\n"
      ],
      "metadata": {
        "id": "iQQxd72LFPDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hparams = genetic_algorithm(y=scaled_close, pop_size=4, generations=2)\n",
        "print(\"📌 Mejores hiperparámetros encontrados:\", best_hparams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uGODB1cQKu-",
        "outputId": "49e8db2e-e942-4f93-9b1b-8de9486f8626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧬 Gen 1/2, Mejor pérdida: 0.000099\n",
            "🧬 Gen 2/2, Mejor pérdida: 0.000041\n",
            "📌 Mejores hiperparámetros encontrados: {'seq_len': 20, 'hidden_size': 32, 'num_layers': 3, 'dropout': 0.2653036041621139, 'lr': 0.0015289256279720606}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ejecutar algoritmo genético\n",
        "# best_hparams = genetic_algorithm(y=scaled_close, pop_size=4, generations=2)\n",
        "# print(\"🧬 Mejores hiperparámetros encontrados:\", best_hparams)"
      ],
      "metadata": {
        "id": "EAPpVKCRFp8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar modelo final\n",
        "# seq_len = best_hparams['seq_len']\n",
        "seq_len = 20\n",
        "X, y_seq = create_sequences(scaled_close, seq_len)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "model = BitcoinLSTM(\n",
        "    input_size=1,\n",
        "    # hidden_size=best_hparams['hidden_size'],\n",
        "    hidden_size=32,\n",
        "    # num_layers=best_hparams['num_layers'],\n",
        "    num_layers=3,\n",
        "    # dropout=best_hparams['dropout']\n",
        "    dropout=0.2653036041621139,\n",
        ")\n",
        "\n",
        "train_model(model, X_train, y_train, X_val, y_val, num_epochs=20,\n",
        "            # lr=best_hparams['lr']\n",
        "            lr=0.0015289256279720606)"
      ],
      "metadata": {
        "id": "6tprGTmjGQsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predicción y visualización\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_val).numpy()\n",
        "\n",
        "actual = scaler.inverse_transform(y_val.numpy())\n",
        "predicted = scaler.inverse_transform(preds)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(actual, label='Real')\n",
        "plt.plot(predicted, label='Predicción')\n",
        "plt.legend()\n",
        "plt.title(\"Predicción del precio de cierre del Bitcoin\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Djpr89OiGVmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Cargar dataset (ajusta el path si es necesario)\n",
        "# df = pd.read_csv(\"/content/btcusd_1-min_data.csv\")\n",
        "\n",
        "# # Preprocesamiento de fecha y cierre\n",
        "# df['Date'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "# df = df.set_index('Date')[['Close']].copy()\n",
        "# df.dropna(inplace=True)\n",
        "\n",
        "# # Escalado\n",
        "# scaler = MinMaxScaler()\n",
        "# scaled_close = scaler.fit_transform(df[['Close']].values).flatten()\n",
        "\n",
        "# # Función para crear secuencias\n",
        "# def create_sequences(data, seq_len):\n",
        "#     X, y = [], []\n",
        "#     for i in range(len(data) - seq_len):\n",
        "#         X.append(data[i:i+seq_len])\n",
        "#         y.append(data[i+seq_len])\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "# # Modelo LSTM\n",
        "# class BitcoinLSTM(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "#         super(BitcoinLSTM, self).__init__()\n",
        "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "#                             dropout=dropout, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out, _ = self.lstm(x)\n",
        "#         return self.fc(out[:, -1, :])\n",
        "\n",
        "# # Entrenamiento\n",
        "# def train_model(model, X_train, y_train, X_val, y_val, num_epochs, lr):\n",
        "#     criterion = nn.MSELoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(X_train)\n",
        "#         loss = criterion(output, y_train)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         preds = model(X_val)\n",
        "#         val_loss = criterion(preds, y_val).item()\n",
        "#     return val_loss\n",
        "\n",
        "# # Algoritmo genético\n",
        "# def genetic_algorithm(y, pop_size=10, generations=5):\n",
        "#     population = []\n",
        "#     for _ in range(pop_size):\n",
        "#         individual = {\n",
        "#             'seq_len': random.randint(10, 50),\n",
        "#             'hidden_size': random.choice([16, 32, 64, 128]),\n",
        "#             'num_layers': random.choice([1, 2, 3]),\n",
        "#             'dropout': random.uniform(0.0, 0.5),\n",
        "#             'lr': random.uniform(0.0005, 0.01)\n",
        "#         }\n",
        "#         population.append(individual)\n",
        "\n",
        "#     for gen in range(generations):\n",
        "#         fitness_scores = []\n",
        "\n",
        "#         for ind in population:\n",
        "#             seq_len = ind['seq_len']\n",
        "#             X, y_seq = create_sequences(y, seq_len)\n",
        "#             if len(X) < 500: continue  # Evita conjuntos muy pequeños\n",
        "#             X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "#             # Tensores\n",
        "#             X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "#             y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "#             X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "#             y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "#             model = BitcoinLSTM(1, ind['hidden_size'], ind['num_layers'], ind['dropout'])\n",
        "#             loss = train_model(model, X_train, y_train, X_val, y_val, num_epochs=5, lr=ind['lr'])\n",
        "#             fitness_scores.append((loss, ind))\n",
        "\n",
        "#         fitness_scores.sort(key=lambda x: x[0])\n",
        "#         best = fitness_scores[:pop_size//2]\n",
        "#         new_population = [ind for _, ind in best]\n",
        "\n",
        "#         # Cruce y mutación\n",
        "#         while len(new_population) < pop_size:\n",
        "#             parent1, parent2 = random.sample(new_population, 2)\n",
        "#             child = {\n",
        "#                 'seq_len': random.choice([parent1['seq_len'], parent2['seq_len']]),\n",
        "#                 'hidden_size': random.choice([parent1['hidden_size'], parent2['hidden_size']]),\n",
        "#                 'num_layers': random.choice([parent1['num_layers'], parent2['num_layers']]),\n",
        "#                 'dropout': max(0.0, min(0.5, (parent1['dropout'] + parent2['dropout']) / 2 + random.uniform(-0.05, 0.05))),\n",
        "#                 'lr': max(0.0001, min(0.01, (parent1['lr'] + parent2['lr']) / 2 + random.uniform(-0.001, 0.001)))\n",
        "#             }\n",
        "#             new_population.append(child)\n",
        "\n",
        "#         population = new_population\n",
        "#         print(f\"Gen {gen+1}/{generations}, Mejor pérdida: {best[0][0]:.6f}\")\n",
        "\n",
        "#     return fitness_scores[0][1]\n",
        "\n",
        "# # Ejecutar algoritmo genético\n",
        "# best_hparams = genetic_algorithm(y=scaled_close, pop_size=6, generations=3)\n",
        "# print(\"🧬 Mejores hiperparámetros encontrados:\", best_hparams)\n",
        "\n",
        "# # Entrenar modelo final\n",
        "# seq_len = best_hparams['seq_len']\n",
        "# X, y_seq = create_sequences(scaled_close, seq_len)\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "# X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "# y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "# X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
        "# y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# model = BitcoinLSTM(\n",
        "#     input_size=1,\n",
        "#     hidden_size=best_hparams['hidden_size'],\n",
        "#     num_layers=best_hparams['num_layers'],\n",
        "#     dropout=best_hparams['dropout']\n",
        "# )\n",
        "\n",
        "# train_model(model, X_train, y_train, X_val, y_val, num_epochs=20, lr=best_hparams['lr'])\n",
        "\n",
        "# # Predicción y visualización\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     preds = model(X_val).numpy()\n",
        "\n",
        "# actual = scaler.inverse_transform(y_val.numpy())\n",
        "# predicted = scaler.inverse_transform(preds)\n",
        "\n",
        "# plt.figure(figsize=(12, 5))\n",
        "# plt.plot(actual, label='Real')\n",
        "# plt.plot(predicted, label='Predicción')\n",
        "# plt.legend()\n",
        "# plt.title(\"Predicción del precio de cierre del Bitcoin\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQYuVi4Wpqj",
        "outputId": "107e8142-0cb9-4e36-cae4-53c420ae8503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.10442988244752732 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Convertir timestamp a datetime\n",
        "df['Date'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "\n",
        "# Reordenar y usar solo 'Date' y 'Close'\n",
        "df = df[['Date', 'Close']].copy()\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "# Opcional: agrupar por hora o día si hay muchas repeticiones\n",
        "df = df.resample('1H', on='Date').mean().dropna()\n",
        "\n",
        "# Normalizar 'Close'\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = scaler.fit_transform(df[['Close']])\n",
        "\n",
        "# Crear secuencias para LSTM\n",
        "def create_sequences(df, seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - seq_len):\n",
        "        X.append(df[i:i+seq_len])\n",
        "        y.append(df[i+seq_len])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X, y = create_sequences(scaled_df, seq_length)\n",
        "\n",
        "# Convertir a tensores\n",
        "import torch\n",
        "X = torch.from_numpy(X).float()\n",
        "y = torch.from_numpy(y).float()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjTgmz6-1eAu",
        "outputId": "eae719ff-0601-43dd-e9b3-7fdb21b6046b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8ef987f2b017>:13: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df = df.resample('1H', on='Date').mean().dropna()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BitcoinLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
        "        super(BitcoinLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # Solo tomamos la salida del último paso de tiempo\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "A-AaAz732uLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, num_epochs=20, lr=0.001):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(X_val)\n",
        "            val_loss = criterion(val_output, y_val)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item():.6f} - Val Loss: {val_loss.item():.6f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "If6d6fQd20vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# División simple\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Inicializar modelo\n",
        "model = BitcoinLSTM()\n",
        "trained_model = train_model(model, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "id": "YdQc9v_A22ra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74239f5a-a275-454b-951f-8808e274bb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.081905 - Val Loss: 0.029647\n",
            "Epoch [2/20] - Loss: 0.074661 - Val Loss: 0.022332\n",
            "Epoch [3/20] - Loss: 0.068413 - Val Loss: 0.015973\n",
            "Epoch [4/20] - Loss: 0.063105 - Val Loss: 0.010588\n",
            "Epoch [5/20] - Loss: 0.058746 - Val Loss: 0.006293\n",
            "Epoch [6/20] - Loss: 0.055429 - Val Loss: 0.003284\n",
            "Epoch [7/20] - Loss: 0.053368 - Val Loss: 0.001772\n",
            "Epoch [8/20] - Loss: 0.052666 - Val Loss: 0.001729\n",
            "Epoch [9/20] - Loss: 0.053051 - Val Loss: 0.002521\n",
            "Epoch [10/20] - Loss: 0.053870 - Val Loss: 0.003171\n",
            "Epoch [11/20] - Loss: 0.054101 - Val Loss: 0.003206\n",
            "Epoch [12/20] - Loss: 0.053375 - Val Loss: 0.002753\n",
            "Epoch [13/20] - Loss: 0.052015 - Val Loss: 0.002147\n",
            "Epoch [14/20] - Loss: 0.050431 - Val Loss: 0.001666\n",
            "Epoch [15/20] - Loss: 0.048873 - Val Loss: 0.001441\n",
            "Epoch [16/20] - Loss: 0.047588 - Val Loss: 0.001468\n",
            "Epoch [17/20] - Loss: 0.046527 - Val Loss: 0.001668\n",
            "Epoch [18/20] - Loss: 0.045652 - Val Loss: 0.001931\n",
            "Epoch [19/20] - Loss: 0.044755 - Val Loss: 0.002154\n",
            "Epoch [20/20] - Loss: 0.043773 - Val Loss: 0.002251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_individual():\n",
        "    return {\n",
        "        'hidden_size': random.choice([16, 32, 64, 128]),\n",
        "        'num_layers': random.choice([1, 2, 3]),\n",
        "        'dropout': random.choice([0.0, 0.2, 0.5]),\n",
        "        'lr': 10 ** random.uniform(-4, -2),  # Log scale: 0.0001 - 0.01\n",
        "        'seq_len': random.choice([10, 20, 30, 50])\n",
        "    }"
      ],
      "metadata": {
        "id": "NDDqRl6XVDWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness(individual, X, y):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import torch\n",
        "\n",
        "    # Repreparar datos según `seq_len`\n",
        "    seq_len = individual['seq_len']\n",
        "    X_seq, y_seq = create_sequences(y, seq_len)  # <-- asume que usas create_sequences\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_seq, y_seq, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Convertir a tensores\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Modelo\n",
        "    model = BitcoinLSTM(\n",
        "        input_size=1,\n",
        "        hidden_size=individual['hidden_size'],\n",
        "        num_layers=individual['num_layers'],\n",
        "        dropout=individual['dropout']\n",
        "    )\n",
        "\n",
        "    trained = train_model(\n",
        "        model, X_train, y_train, X_val, y_val,\n",
        "        num_epochs=5,  # entrenamiento corto por eficiencia\n",
        "        lr=individual['lr']\n",
        "    )\n",
        "\n",
        "    # Evaluar fitness\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_output = model(X_val)\n",
        "        mse = nn.MSELoss()(val_output, y_val)\n",
        "\n",
        "    return mse.item()"
      ],
      "metadata": {
        "id": "8tr1hwrIVHiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm(X, y, pop_size=10, generations=5):\n",
        "    population = [generate_individual() for _ in range(pop_size)]\n",
        "\n",
        "    for generation in range(generations):\n",
        "        print(f\"Generación {generation+1}/{generations}\")\n",
        "        fitness_scores = [fitness(ind, X, y) for ind in population]\n",
        "\n",
        "        # Seleccionar los mejores\n",
        "        sorted_population = [x for _, x in sorted(zip(fitness_scores, population))]\n",
        "        population = sorted_population[:pop_size//2]  # top 50%\n",
        "\n",
        "        # Reproducir\n",
        "        children = []\n",
        "        while len(children) < pop_size // 2:\n",
        "            parent1 = random.choice(population)\n",
        "            parent2 = random.choice(population)\n",
        "\n",
        "            # Cruce simple\n",
        "            child = {\n",
        "                key: random.choice([parent1[key], parent2[key]])\n",
        "                for key in parent1\n",
        "            }\n",
        "\n",
        "            # Mutación\n",
        "            if random.random() < 0.3:\n",
        "                child = mutate(child)\n",
        "\n",
        "            children.append(child)\n",
        "\n",
        "        population += children\n",
        "\n",
        "    # Última evaluación\n",
        "    final_scores = [fitness(ind, X, y) for ind in population]\n",
        "    best_idx = final_scores.index(min(final_scores))\n",
        "    best_individual = population[best_idx]\n",
        "\n",
        "    print(f\"Mejores hiperparámetros encontrados: {best_individual}\")\n",
        "    return best_individual"
      ],
      "metadata": {
        "id": "2Uu9C87bVqmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mutate(individual):\n",
        "    mutated = individual.copy()\n",
        "    key = random.choice(list(mutated.keys()))\n",
        "    if key == 'hidden_size':\n",
        "        mutated[key] = random.choice([16, 32, 64, 128])\n",
        "    elif key == 'num_layers':\n",
        "        mutated[key] = random.choice([1, 2, 3])\n",
        "    elif key == 'dropout':\n",
        "        mutated[key] = random.choice([0.0, 0.2, 0.5])\n",
        "    elif key == 'lr':\n",
        "        mutated[key] = 10 ** random.uniform(-4, -2)\n",
        "    elif key == 'seq_len':\n",
        "        mutated[key] = random.choice([10, 20, 30, 50])\n",
        "    return mutated"
      ],
      "metadata": {
        "id": "cn_wM2fbVuhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}